import os
import json
import requests
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
import gspread
from oauth2client.service_account import ServiceAccountCredentials
import feedparser
from urllib.parse import quote_plus
from pymorphy2 import MorphAnalyzer

print("‚úÖ –°–∫—Ä–∏–ø—Ç main.py –∑–∞–ø—É—â–µ–Ω", flush=True)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ—Ä—Ñ–æ–∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
morph = MorphAnalyzer()

def normalize(text):
    words = text.lower().split()
    return ' '.join([morph.parse(w)[0].normal_form for w in words])

# –ó–∞–≥—Ä—É–∑–∫–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
with open("keywords.txt", "r", encoding="utf-8") as f:
    KEYWORDS = [line.strip() for line in f if line.strip()]
print(f"üîë –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(KEYWORDS)} –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤", flush=True)

# –î–∞—Ç–∞: –≤—á–µ—Ä–∞
yesterday = (datetime.utcnow() + timedelta(hours=3) - timedelta(days=1)).strftime('%Y-%m-%d')

# –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –≤ Google Sheets
try:
    scope = ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]
    creds_json = os.getenv("GOOGLE_CREDS_JSON")
    creds_dict = json.loads(creds_json)
    creds = ServiceAccountCredentials.from_json_keyfile_dict(creds_dict, scope)
    client = gspread.authorize(creds)
    sheet = client.open_by_url("https://docs.google.com/spreadsheets/d/1ar4pf2_6zqmplMAFFuB2BsolhpHh00jFvn5kmigaVlE/edit")
    worksheet = sheet.sheet1
    print("üìó –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –≤ Google Sheets —É—Å–ø–µ—à–Ω–∞", flush=True)
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –≤ Google Sheets: {e}", flush=True)
    exit(1)

existing_links = {
    row[2] for row in worksheet.get_all_values() if len(row) > 2 and row[2]
}

# --- –ü–æ–∏—Å–∫ –≤ –Ø–Ω–¥–µ–∫—Å–µ
def search_yandex_news(query):
    print(f"üîé –Ø–Ω–¥–µ–∫—Å: {query}", flush=True)
    url = f"https://yandex.ru/news/search?text={quote_plus(query)}&from=day"
    headers = {"User-Agent": "Mozilla/5.0"}
    resp = requests.get(url, headers=headers)
    soup = BeautifulSoup(resp.text, "html.parser")
    results = []
    for item in soup.select("article"):
        title_tag = item.find("h2")
        link_tag = item.find("a")
        if title_tag and link_tag:
            title = title_tag.get_text(strip=True)
            link = link_tag.get("href")
            if link and link.startswith("/news"):
                link = "https://yandex.ru" + link
            results.append((title, link))
    return results

# --- –ü–æ–∏—Å–∫ –≤ Google News
def search_google_news(query):
    print(f"üîé Google News: {query}", flush=True)
    encoded_query = quote_plus(f"{query} when:1d location:Russia")
    url = f"https://news.google.com/rss/search?q={encoded_query}&hl=ru&gl=RU&ceid=RU:ru"
    feed = feedparser.parse(url)
    return [(entry.title, entry.link) for entry in feed.entries]

# --- –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –ø–æ –ª–µ–º–º–∞–º
def is_match(keyword, title):
    return normalize(keyword) in normalize(title)

# --- –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª
found_rows = []
for keyword in KEYWORDS:
    yandex_results = search_yandex_news(keyword)
    google_results = search_google_news(keyword)
    combined = yandex_results + google_results
    new_items = 0

    for title, link in combined:
        if link in existing_links:
            continue
        if not is_match(keyword, title):
            continue
        found_rows.append([yesterday, title, link, "", keyword, "–î–∞"])
        existing_links.add(link)
        new_items += 1

    print(f"üìå {keyword} ‚Äî –Ω–æ–≤—ã—Ö: {new_items}, –≤—Å–µ–≥–æ –ø–æ–ª—É—á–µ–Ω–æ: {len(combined)}", flush=True)

# --- –ó–∞–ø–∏—Å—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
if found_rows:
    try:
        worksheet.append_rows(found_rows)
        print(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {len(found_rows)} —Å—Ç—Ä–æ–∫ –≤ Google Sheets", flush=True)
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ –≤ Google Sheets: {e}", flush=True)
else:
    worksheet.append_row([yesterday, "–ù–µ—Ç –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º", "", "", "", "–ù–µ—Ç"])
    print("üì≠ –ù–æ–≤–æ—Å—Ç–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ ‚Äî –¥–æ–±–∞–≤–ª–µ–Ω–∞ —Å—Ç—Ä–æ–∫–∞-–∑–∞–≥–ª—É—à–∫–∞", flush=True)

# --- Telegram —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ
def send_telegram_message(text):
    bot_token = os.getenv("TELEGRAM_BOT_TOKEN")
    chat_id = os.getenv("TELEGRAM_CHAT_ID")
    if not bot_token or not chat_id:
        print("‚ö†Ô∏è –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ Telegram", flush=True)
        return
    url = f"https://api.telegram.org/bot{bot_token}/sendMessage"
    payload = {"chat_id": chat_id, "text": text}
    try:
        resp = requests.post(url, json=payload)
        print(f"üì¨ Telegram —Å—Ç–∞—Ç—É—Å: {resp.status_code}, –æ—Ç–≤–µ—Ç: {resp.text}", flush=True)
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ Telegram: {e}", flush=True)

# –û—Ç–ø—Ä–∞–≤–∫–∞ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è
if found_rows:
    send_telegram_message(f"üì∞ –î–æ–±–∞–≤–ª–µ–Ω–æ {len(found_rows)} –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ {yesterday}")
else:
    send_telegram_message(f"üì≠ –ó–∞ {yesterday} –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
