import os
import json
import datetime
import requests
import feedparser
import gspread
from bs4 import BeautifulSoup
from oauth2client.service_account import ServiceAccountCredentials
from urllib.parse import quote_plus
import pymorphy2

# ğŸ“… Ğ”Ğ°Ñ‚Ğ° Ğ¿Ğ¾Ğ¸ÑĞºĞ°: Ğ²Ñ‡ĞµÑ€Ğ°
yesterday = (datetime.datetime.now() - datetime.timedelta(days=1)).strftime("%Y-%m-%d")
print("âœ… Ğ¡ĞºÑ€Ğ¸Ğ¿Ñ‚ main.py Ğ·Ğ°Ğ¿ÑƒÑ‰ĞµĞ½")

# ğŸ“ Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ»Ğ¾Ğ²
with open("keywords.txt", "r", encoding="utf-8") as f:
    KEYWORDS = [line.strip() for line in f if line.strip()]
print(f"ğŸ”‘ Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¾ {len(KEYWORDS)} ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ»Ğ¾Ğ²")

# ğŸ“„ ĞĞ²Ñ‚Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Google Sheets
scope = ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]
creds_json = json.loads(os.environ["GOOGLE_CREDS_JSON"])
creds = ServiceAccountCredentials.from_json_keyfile_dict(creds_json, scope)
client = gspread.authorize(creds)
sheet = client.open_by_url(os.environ["SHEET_URL"]).sheet1
print("ğŸ“— ĞĞ²Ñ‚Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ² Google Sheets ÑƒÑĞ¿ĞµÑˆĞ½Ğ°")

# ğŸ§  ĞœĞ¾Ñ€Ñ„Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ñ€Ğ°Ğ·Ğ±Ğ¾Ñ€
morph = pymorphy2.MorphAnalyzer()
def normalize_text(text):
    return " ".join([morph.parse(word)[0].normal_form for word in text.lower().split()])

def is_relevant(title, keyword):
    norm_title = normalize_text(title)
    norm_keyword = normalize_text(keyword)
    return norm_keyword in norm_title

# ğŸ” ĞŸĞ¾Ğ¸ÑĞº Ğ² Ğ¯Ğ½Ğ´ĞµĞºÑĞµ
def search_yandex_news(query):
    url = f"https://yandex.ru/news/search?text={quote_plus(query)}&rdr=1&lr=213"
    headers = {"User-Agent": "Mozilla/5.0"}
    resp = requests.get(url, headers=headers)
    soup = BeautifulSoup(resp.text, "html.parser")
    results = []
    for item in soup.select("article"):
        title_tag = item.find("h2")
        link_tag = item.find("a")
        if title_tag and link_tag:
            title = title_tag.get_text(strip=True)
            link = link_tag.get("href")
            if link and link.startswith("/news"):
                link = "https://yandex.ru" + link
            results.append((title, link))
    return results

# ğŸ” ĞŸĞ¾Ğ¸ÑĞº Ğ² Google News
def search_google_news(query):
    cleaned_query = query.strip().replace('\n', ' ').replace('\r', ' ')
    encoded_query = quote_plus(cleaned_query)
    url = f"https://news.google.com/rss/search?q={encoded_query}+when:{yesterday}&hl=ru&gl=RU&ceid=RU:ru"
    feed = feedparser.parse(url)
    return [(entry.title, entry.link) for entry in feed.entries]

# ğŸ“‹ Ğ§Ñ‚ĞµĞ½Ğ¸Ğµ ÑƒĞ¶Ğµ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚ĞµĞ¹
sent_links = set()
if os.path.exists("sent_posts.json"):
    with open("sent_posts.json", "r", encoding="utf-8") as f:
        try:
            sent_links = set(json.load(f))
        except json.JSONDecodeError:
            pass

def save_and_log(results, keyword):
    count = 0
    for title, link in results:
        if link not in sent_links:
            sheet.append_row([yesterday, keyword, title, link])
            sent_links.add(link)
            count += 1
    return count

# ğŸš€ ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ñ†Ğ¸ĞºĞ» Ğ¿Ğ¾ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ ÑĞ»Ğ¾Ğ²Ğ°Ğ¼
found_links = []
print(f"ğŸ” ĞŸĞ¾Ğ¸ÑĞº Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚ĞµĞ¹ Ğ·Ğ° {yesterday}")

for keyword in KEYWORDS:
    print(f"ğŸ” Ğ¯Ğ½Ğ´ĞµĞºÑ: {keyword}")
    yandex_results = search_yandex_news(keyword)

    print(f"ğŸ” Google News: {keyword}")
    google_results = search_google_news(keyword)

    combined = yandex_results + google_results
    found_count = len(combined)

    # ğŸ“ˆ Ğ›Ğ¾Ğ³Ğ¸Ñ€ÑƒĞµĞ¼ Ğ²ÑĞµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ½Ñ‹Ğµ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸
    for source, results in [("ğŸŸ¡ Ğ¯Ğ½Ğ´ĞµĞºÑ", yandex_results), ("ğŸ”µ Google", google_results)]:
        for title, link in results:
            print(f"{source} â¤ {title}\n   â†ª {link}")

    # ğŸ§° ĞœĞ¾Ñ€Ñ„Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ
    filtered_results = []
    for title, link in combined:
        if is_relevant(title, keyword):
            filtered_results.append((title, link))

    # ğŸ“Š Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ½Ğ¾Ğ²Ñ‹Ğµ
    new_count = save_and_log(filtered_results, keyword)
    print(f"ğŸ“Œ {keyword} â€” Ğ½Ğ¾Ğ²Ñ‹Ñ…: {new_count}, Ğ²ÑĞµĞ³Ğ¾ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾: {found_count}\n")

# ğŸ“† Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ²ÑĞµ ÑÑÑ‹Ğ»ĞºĞ¸
with open("sent_posts.json", "w", encoding="utf-8") as f:
    json.dump(list(sent_links), f, ensure_ascii=False, indent=2)

# ğŸ“¨ Ğ•ÑĞ»Ğ¸ Ğ½Ğ¸Ñ‡ĞµĞ³Ğ¾ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾ â€” Ğ¿Ğ¸ÑˆĞµĞ¼ Ğ² Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñƒ
if not found_links:
    sheet.append_row([yesterday, "ĞĞµÑ‚ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚ĞµĞ¹", "", ""])
    print("ğŸ‘­ ĞĞ¾Ğ²Ğ¾ÑÑ‚ĞµĞ¹ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾ â€” Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑÑ‚Ñ€Ğ¾ĞºĞ°-Ğ·Ğ°Ğ³Ğ»ÑƒÑˆĞºĞ°")
