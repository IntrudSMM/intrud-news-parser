import os
import json
from dotenv import load_dotenv
load_dotenv()

import datetime
import requests
import feedparser
import gspread
from bs4 import BeautifulSoup
from oauth2client.service_account import ServiceAccountCredentials
from urllib.parse import quote_plus
import pymorphy2
import telegram

# üìÖ –î–∞—Ç–∞ –ø–æ–∏—Å–∫–∞: –≤—á–µ—Ä–∞
yesterday = (datetime.datetime.now() - datetime.timedelta(days=1)).strftime("%Y-%m-%d")
print("‚úÖ –°–∫—Ä–∏–ø—Ç main.py –∑–∞–ø—É—â–µ–Ω")

# üìÅ –ó–∞–≥—Ä—É–∑–∫–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
with open("keywords.txt", "r", encoding="utf-8") as f:
    KEYWORDS = [line.strip() for line in f if line.strip()]
print(f"üîë –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(KEYWORDS)} –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤")

# üìÑ –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è Google Sheets
scope = ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]
creds_json = json.loads(os.environ["GOOGLE_CREDS_JSON"])
creds = ServiceAccountCredentials.from_json_keyfile_dict(creds_json, scope)
client = gspread.authorize(creds)
sheet = client.open_by_key(os.environ["SPREADSHEET_ID"]).sheet1
print("üìó –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –≤ Google Sheets —É—Å–ø–µ—à–Ω–∞")

bot = None
chat_id = os.environ.get("TELEGRAM_CHAT_ID")
token = os.environ.get("TELEGRAM_BOT_TOKEN")

if chat_id and token:
    bot = telegram.Bot(token=token)


# üß† –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π —Ä–∞–∑–±–æ—Ä
morph = pymorphy2.MorphAnalyzer()
def normalize_text(text):
    return " ".join([morph.parse(word)[0].normal_form for word in text.lower().split()])

def is_relevant(title, keyword):
    norm_title = normalize_text(title)
    norm_keyword = normalize_text(keyword)
    return norm_keyword in norm_title

# üîç –ü–æ–∏—Å–∫ –≤ –Ø–Ω–¥–µ–∫—Å–µ
def search_yandex_news(query):
    url = f"https://yandex.ru/news/search?text={quote_plus(query)}&rdr=1&lr=213"
    headers = {"User-Agent": "Mozilla/5.0"}
    resp = requests.get(url, headers=headers)
    soup = BeautifulSoup(resp.text, "html.parser")
    results = []

    for article in soup.select("article"):
        title_tag = article.find("h2")
        link_tag = article.find("a")

        if title_tag and link_tag:
            title = title_tag.get_text(strip=True)
            yandex_link = link_tag.get("href")

            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤–Ω—É—Ç—Ä–µ–Ω–Ω—é—é —Å—Å—ã–ª–∫—É –≤ –≤–Ω–µ—à–Ω—é—é
            if yandex_link and yandex_link.startswith("/news"):
                full_yandex_url = "https://yandex.ru" + yandex_link
                try:
                    preview = requests.get(full_yandex_url, headers=headers, timeout=5)
                    preview_soup = BeautifulSoup(preview.text, "html.parser")
                    real_link = None

                    for tag in preview_soup.find_all("a", href=True):
                        if "yandex.ru" not in tag["href"] and tag["href"].startswith("http"):
                            real_link = tag["href"]
                            break

                    if real_link:
                        results.append((title, real_link))
                    else:
                        results.append((title, full_yandex_url))  # fallback

                except Exception as e:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –≤–Ω–µ—à–Ω–µ–π —Å—Å—ã–ª–∫–∏ –Ø–Ω–¥–µ–∫—Å–∞: {e}")
                    results.append((title, full_yandex_url))
            else:
                results.append((title, yandex_link))

    return results


# üîç –ü–æ–∏—Å–∫ –≤ Google News
def search_google_news(query):
    cleaned_query = query.strip().replace('\n', ' ').replace('\r', ' ')
    encoded_query = quote_plus(cleaned_query)
    url = f"https://news.google.com/rss/search?q={encoded_query}+when:{yesterday}&hl=ru&gl=RU&ceid=RU:ru"
    feed = feedparser.parse(url)
    return [(entry.title, entry.link) for entry in feed.entries]

# üìã –ß—Ç–µ–Ω–∏–µ —É–∂–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
sent_links = set()
if os.path.exists("sent_posts.json"):
    with open("sent_posts.json", "r", encoding="utf-8") as f:
        try:
            sent_links = set(json.load(f))
        except json.JSONDecodeError:
            pass

def save_and_log(results, keyword):
    saved = []
    for title, link in results:
        if link not in sent_links:
            sheet.append_row([yesterday, keyword, title, link])
            sent_links.add(link)
            saved.append((title, link, keyword))
    return saved

# üöÄ –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
found_links = []
print(f"üîç –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ {yesterday}")

for keyword in KEYWORDS:
    print(f"üîé –Ø–Ω–¥–µ–∫—Å: {keyword}")
    yandex_results = search_yandex_news(keyword)

    print(f"üîé Google News: {keyword}")
    google_results = search_google_news(keyword)

    combined = yandex_results + google_results
    found_count = len(combined)

    # üìà –õ–æ–≥–∏—Ä—É–µ–º –≤—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏
    for source, results in [("üü° –Ø–Ω–¥–µ–∫—Å", yandex_results), ("üîµ Google", google_results)]:
        for title, link in results:
            print(f"{source} ‚û§ {title}\n   ‚Ü™ {link}")

    # üß∞ –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è
    filtered_results = []
    for title, link in combined:
        if is_relevant(title, keyword):
            filtered_results.append((title, link))

    # üìä –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–æ–≤—ã–µ
    new_items = save_and_log(filtered_results, keyword)
    found_links.extend(new_items)
    print(f"üìå {keyword} ‚Äî –Ω–æ–≤—ã—Ö: {len(new_items)}, –≤—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ: {found_count}\n")


# üìÜ –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏
with open("sent_posts.json", "w", encoding="utf-8") as f:
    json.dump(list(sent_links), f, ensure_ascii=False, indent=2)

# üì® –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ ‚Äî –ø–∏—à–µ–º –≤ —Ç–∞–±–ª–∏—Ü—É
if not found_links:
    sheet.append_row([yesterday, "–ù–µ—Ç –Ω–æ–≤–æ—Å—Ç–µ–π", "", ""])
    print("üë≠ –ù–æ–≤–æ—Å—Ç–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ ‚Äî –¥–æ–±–∞–≤–ª–µ–Ω–∞ —Å—Ç—Ä–æ–∫–∞-–∑–∞–≥–ª—É—à–∫–∞")

import re
def escape_markdown(text):
    return re.sub(r'([_*\[\]()~`>#+\-=|{}.!])', r'\\\1', text)

if bot:
    if found_links:
        max_length = 4096
        header = f"üóû –ù–æ–≤–æ—Å—Ç–∏ –∑–∞ {yesterday}:\n\n"
        footer = f"\nüìä –í—Å–µ–≥–æ –Ω–æ–≤–æ—Å—Ç–µ–π: {len(found_links)}"
        message_blocks = []
        current_block = header
        i = 1

        for title, link, keyword in found_links:
            entry = f"{i}. üì∞ *{escape_markdown(title)}*\nüîó [–ß–∏—Ç–∞—Ç—å —Å—Ç–∞—Ç—å—é]({link})\nüè∑ –ö–ª—é—á: `{escape_markdown(keyword)}`\n\n"
            if len(current_block) + len(entry) + len(footer) > max_length:
                message_blocks.append(current_block)
                current_block = ""
            current_block += entry
            i += 1

        if current_block:
            message_blocks.append(current_block)

        for j, block in enumerate(message_blocks):
            final_text = block
            if j == len(message_blocks) - 1:
                final_text += footer
            try:
                bot.send_message(chat_id=chat_id, text=final_text, parse_mode=telegram.ParseMode.MARKDOWN)
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ —Å–æ–æ–±—â–µ–Ω–∏—è {j+1}: {e}")
    else:
        bot.send_message(chat_id=chat_id, text=f"üì≠ –ó–∞ {yesterday} –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")
else:
    print("‚ö†Ô∏è Telegram –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –Ω–µ –∑–∞–¥–∞–Ω—ã")

